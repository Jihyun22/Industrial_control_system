{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "import dateutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/training/train1.csv'),\n",
       " WindowsPath('data/training/train2.csv'),\n",
       " WindowsPath('data/training/train3.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#트레인\n",
    "TRAIN_DATASET = sorted([x for x in Path(\"data/training/\").glob(\"*.csv\")])\n",
    "TRAIN_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/testing/test1.csv'),\n",
       " WindowsPath('data/testing/test2.csv'),\n",
       " WindowsPath('data/testing/test3.csv'),\n",
       " WindowsPath('data/testing/test4.csv')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 셋\n",
    "TEST_DATASET = sorted([x for x in Path(\"data/testing/\").glob(\"*.csv\")])\n",
    "TEST_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 셋 -> 보조자료\n",
    "VALIDATION_DATASET = sorted([x for x in Path(\"data/validation/\").glob(\"*.csv\")])\n",
    "VALIDATION_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링 1차\n",
    "30일 간 데이터에 대한 모델 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\jih02\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.16.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4320, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nan 값 제거\n",
    "y = y_train.dropna()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4320, 40)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트레인 셋 범주 조정\n",
    "X = X_train.loc[:y.shape[0]-1,:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20)                1760      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                378       \n",
      "=================================================================\n",
      "Total params: 2,138\n",
      "Trainable params: 2,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential() # Sequeatial Model\n",
    "model.add(LSTM(20, input_shape=(X.shape[1], 1))) # 트레인값\n",
    "model.add(Dense(y.shape[1])) # 출력값\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4320/4320 [==============================] - ETA: 2s - loss: 6.129 - ETA: 2s - loss: 5.834 - ETA: 2s - loss: 6.429 - ETA: 2s - loss: 6.174 - ETA: 2s - loss: 6.290 - ETA: 2s - loss: 6.511 - ETA: 2s - loss: 6.523 - ETA: 2s - loss: 6.435 - ETA: 1s - loss: 6.605 - ETA: 1s - loss: 6.545 - ETA: 1s - loss: 6.723 - ETA: 1s - loss: 6.894 - ETA: 1s - loss: 6.957 - ETA: 1s - loss: 7.069 - ETA: 1s - loss: 7.074 - ETA: 1s - loss: 7.054 - ETA: 1s - loss: 7.050 - ETA: 1s - loss: 7.099 - ETA: 1s - loss: 7.165 - ETA: 1s - loss: 7.198 - ETA: 1s - loss: 7.192 - ETA: 1s - loss: 7.131 - ETA: 1s - loss: 7.124 - ETA: 1s - loss: 7.073 - ETA: 0s - loss: 7.074 - ETA: 0s - loss: 7.111 - ETA: 0s - loss: 7.071 - ETA: 0s - loss: 7.098 - ETA: 0s - loss: 7.100 - ETA: 0s - loss: 7.077 - ETA: 0s - loss: 7.047 - ETA: 0s - loss: 7.012 - ETA: 0s - loss: 6.992 - ETA: 0s - loss: 6.980 - ETA: 0s - loss: 6.970 - ETA: 0s - loss: 6.938 - ETA: 0s - loss: 6.913 - ETA: 0s - loss: 6.902 - ETA: 0s - loss: 6.921 - 2s 545us/step - loss: 6.9204\n",
      "Epoch 2/1000\n",
      "4320/4320 [==============================] - ETA: 1s - loss: 5.736 - ETA: 2s - loss: 7.219 - ETA: 2s - loss: 6.753 - ETA: 1s - loss: 6.834 - ETA: 1s - loss: 6.985 - ETA: 1s - loss: 7.032 - ETA: 1s - loss: 6.949 - ETA: 1s - loss: 6.800 - ETA: 1s - loss: 6.857 - ETA: 1s - loss: 6.909 - ETA: 1s - loss: 6.969 - ETA: 1s - loss: 6.933 - ETA: 1s - loss: 6.951 - ETA: 1s - loss: 6.887 - ETA: 1s - loss: 6.802 - ETA: 1s - loss: 6.758 - ETA: 1s - loss: 6.772 - ETA: 1s - loss: 6.812 - ETA: 1s - loss: 6.795 - ETA: 1s - loss: 6.745 - ETA: 1s - loss: 6.735 - ETA: 1s - loss: 6.738 - ETA: 1s - loss: 6.699 - ETA: 0s - loss: 6.671 - ETA: 0s - loss: 6.725 - ETA: 0s - loss: 6.787 - ETA: 0s - loss: 6.779 - ETA: 0s - loss: 6.721 - ETA: 0s - loss: 6.721 - ETA: 0s - loss: 6.760 - ETA: 0s - loss: 6.727 - ETA: 0s - loss: 6.719 - ETA: 0s - loss: 6.741 - ETA: 0s - loss: 6.728 - ETA: 0s - loss: 6.709 - ETA: 0s - loss: 6.714 - ETA: 0s - loss: 6.725 - ETA: 0s - loss: 6.756 - ETA: 0s - loss: 6.750 - ETA: 0s - loss: 6.759 - 2s 538us/step - loss: 6.7618\n",
      "Epoch 3/1000\n",
      "4320/4320 [==============================] - ETA: 3s - loss: 5.943 - ETA: 2s - loss: 5.927 - ETA: 2s - loss: 6.164 - ETA: 2s - loss: 6.301 - ETA: 2s - loss: 6.259 - ETA: 2s - loss: 6.231 - ETA: 2s - loss: 6.123 - ETA: 1s - loss: 5.999 - ETA: 1s - loss: 6.145 - ETA: 1s - loss: 6.382 - ETA: 1s - loss: 6.468 - ETA: 1s - loss: 6.509 - ETA: 1s - loss: 6.542 - ETA: 1s - loss: 6.592 - ETA: 1s - loss: 6.549 - ETA: 1s - loss: 6.524 - ETA: 1s - loss: 6.525 - ETA: 1s - loss: 6.562 - ETA: 1s - loss: 6.524 - ETA: 1s - loss: 6.471 - ETA: 1s - loss: 6.519 - ETA: 1s - loss: 6.596 - ETA: 0s - loss: 6.577 - ETA: 0s - loss: 6.643 - ETA: 0s - loss: 6.659 - ETA: 0s - loss: 6.661 - ETA: 0s - loss: 6.614 - ETA: 0s - loss: 6.670 - ETA: 0s - loss: 6.721 - ETA: 0s - loss: 6.763 - ETA: 0s - loss: 6.809 - ETA: 0s - loss: 6.813 - ETA: 0s - loss: 6.783 - ETA: 0s - loss: 6.816 - ETA: 0s - loss: 6.792 - ETA: 0s - loss: 6.748 - ETA: 0s - loss: 6.721 - ETA: 0s - loss: 6.732 - ETA: 0s - loss: 6.715 - 2s 534us/step - loss: 6.7303\n",
      "Epoch 4/1000\n",
      "4320/4320 [==============================] - ETA: 2s - loss: 6.840 - ETA: 2s - loss: 8.263 - ETA: 2s - loss: 7.192 - ETA: 2s - loss: 7.161 - ETA: 2s - loss: 7.018 - ETA: 2s - loss: 6.912 - ETA: 2s - loss: 6.780 - ETA: 1s - loss: 6.678 - ETA: 1s - loss: 6.892 - ETA: 1s - loss: 6.862 - ETA: 1s - loss: 6.761 - ETA: 1s - loss: 6.766 - ETA: 1s - loss: 6.730 - ETA: 1s - loss: 6.741 - ETA: 1s - loss: 6.745 - ETA: 1s - loss: 6.764 - ETA: 1s - loss: 6.766 - ETA: 1s - loss: 6.692 - ETA: 1s - loss: 6.689 - ETA: 1s - loss: 6.673 - ETA: 1s - loss: 6.663 - ETA: 1s - loss: 6.670 - ETA: 1s - loss: 6.592 - ETA: 0s - loss: 6.596 - ETA: 0s - loss: 6.612 - ETA: 0s - loss: 6.601 - ETA: 0s - loss: 6.673 - ETA: 0s - loss: 6.658 - ETA: 0s - loss: 6.618 - ETA: 0s - loss: 6.621 - ETA: 0s - loss: 6.619 - ETA: 0s - loss: 6.580 - ETA: 0s - loss: 6.576 - ETA: 0s - loss: 6.584 - ETA: 0s - loss: 6.590 - ETA: 0s - loss: 6.587 - ETA: 0s - loss: 6.601 - ETA: 0s - loss: 6.593 - ETA: 0s - loss: 6.641 - ETA: 0s - loss: 6.634 - 2s 527us/step - loss: 6.6412\n",
      "Epoch 5/1000\n",
      "4320/4320 [==============================] - ETA: 2s - loss: 8.065 - ETA: 2s - loss: 6.238 - ETA: 2s - loss: 6.908 - ETA: 2s - loss: 6.867 - ETA: 2s - loss: 6.858 - ETA: 2s - loss: 6.938 - ETA: 2s - loss: 6.724 - ETA: 2s - loss: 6.677 - ETA: 1s - loss: 6.548 - ETA: 1s - loss: 6.400 - ETA: 1s - loss: 6.462 - ETA: 1s - loss: 6.404 - ETA: 1s - loss: 6.396 - ETA: 1s - loss: 6.350 - ETA: 1s - loss: 6.272 - ETA: 1s - loss: 6.333 - ETA: 1s - loss: 6.273 - ETA: 1s - loss: 6.304 - ETA: 1s - loss: 6.334 - ETA: 1s - loss: 6.349 - ETA: 1s - loss: 6.329 - ETA: 1s - loss: 6.436 - ETA: 1s - loss: 6.472 - ETA: 0s - loss: 6.450 - ETA: 0s - loss: 6.500 - ETA: 0s - loss: 6.506 - ETA: 0s - loss: 6.520 - ETA: 0s - loss: 6.535 - ETA: 0s - loss: 6.585 - ETA: 0s - loss: 6.599 - ETA: 0s - loss: 6.631 - ETA: 0s - loss: 6.688 - ETA: 0s - loss: 6.668 - ETA: 0s - loss: 6.703 - ETA: 0s - loss: 6.675 - ETA: 0s - loss: 6.663 - ETA: 0s - loss: 6.669 - ETA: 0s - loss: 6.668 - ETA: 0s - loss: 6.640 - ETA: 0s - loss: 6.612 - 2s 537us/step - loss: 6.6226\n",
      "Epoch 6/1000\n",
      "4320/4320 [==============================] - ETA: 2s - loss: 8.254 - ETA: 2s - loss: 6.646 - ETA: 2s - loss: 6.345 - ETA: 2s - loss: 5.838 - ETA: 2s - loss: 6.134 - ETA: 2s - loss: 6.127 - ETA: 2s - loss: 6.309 - ETA: 2s - loss: 6.192 - ETA: 2s - loss: 6.206 - ETA: 2s - loss: 6.094 - ETA: 1s - loss: 6.187 - ETA: 1s - loss: 6.229 - ETA: 1s - loss: 6.118 - ETA: 1s - loss: 6.178 - ETA: 1s - loss: 6.223 - ETA: 1s - loss: 6.390 - ETA: 1s - loss: 6.484 - ETA: 1s - loss: 6.478 - ETA: 1s - loss: 6.570 - ETA: 1s - loss: 6.601 - ETA: 1s - loss: 6.589 - ETA: 1s - loss: 6.602 - ETA: 1s - loss: 6.579 - ETA: 1s - loss: 6.547 - ETA: 0s - loss: 6.511 - ETA: 0s - loss: 6.510 - ETA: 0s - loss: 6.571 - ETA: 0s - loss: 6.562 - ETA: 0s - loss: 6.566 - ETA: 0s - loss: 6.619 - ETA: 0s - loss: 6.621 - ETA: 0s - loss: 6.624 - ETA: 0s - loss: 6.612 - ETA: 0s - loss: 6.595 - ETA: 0s - loss: 6.596 - ETA: 0s - loss: 6.607 - ETA: 0s - loss: 6.579 - ETA: 0s - loss: 6.593 - ETA: 0s - loss: 6.580 - ETA: 0s - loss: 6.604 - ETA: 0s - loss: 6.577 - 2s 546us/step - loss: 6.5608\n",
      "Epoch 7/1000\n",
      "4320/4320 [==============================] - ETA: 2s - loss: 6.599 - ETA: 2s - loss: 6.860 - ETA: 2s - loss: 7.473 - ETA: 2s - loss: 6.896 - ETA: 1s - loss: 6.553 - ETA: 1s - loss: 6.732 - ETA: 1s - loss: 6.769 - ETA: 1s - loss: 6.709 - ETA: 1s - loss: 6.689 - ETA: 1s - loss: 6.694 - ETA: 1s - loss: 6.594 - ETA: 1s - loss: 6.631 - ETA: 1s - loss: 6.602 - ETA: 1s - loss: 6.626 - ETA: 1s - loss: 6.565 - ETA: 1s - loss: 6.544 - ETA: 1s - loss: 6.526 - ETA: 1s - loss: 6.539 - ETA: 1s - loss: 6.552 - ETA: 1s - loss: 6.521 - ETA: 1s - loss: 6.588 - ETA: 1s - loss: 6.629 - ETA: 1s - loss: 6.576 - ETA: 0s - loss: 6.566 - ETA: 0s - loss: 6.526 - ETA: 0s - loss: 6.549 - ETA: 0s - loss: 6.548 - ETA: 0s - loss: 6.581 - ETA: 0s - loss: 6.562 - ETA: 0s - loss: 6.612 - ETA: 0s - loss: 6.585 - ETA: 0s - loss: 6.658 - ETA: 0s - loss: 6.669 - ETA: 0s - loss: 6.667 - ETA: 0s - loss: 6.652 - ETA: 0s - loss: 6.666 - ETA: 0s - loss: 6.686 - ETA: 0s - loss: 6.663 - ETA: 0s - loss: 6.671 - ETA: 0s - loss: 6.640 - 2s 535us/step - loss: 6.6394\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x124bae93e08>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)\n",
    "\n",
    "model_1.fit(X, y, epochs=1000,\n",
    "          batch_size=30, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_1.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 저장\n",
    "import joblib\n",
    "joblib.dump(model_1, 'model_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 적용 함수\n",
    "def model_fit_1(data, model_name, file_name):\n",
    "    model = joblib.load(str(model_name)) \n",
    "    data_r = data.values.reshape(data.shape[0],data.shape[1],1 )\n",
    "    pred_out=model.predict(data_r)\n",
    "    df = pd.DataFrame(pred_out)\n",
    "    df.to_csv(str(file_name), index = False, header = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#지난 3일간의 데이터셋\n",
    "X_test = X_train.loc[y.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#지난3일간의 y값 연산\n",
    "pred_out = model_fit_1(X_test, 'model_1.pkl', 'pred_out_last3dayy0017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링 2차\n",
    "3일간의 데이터 y값과 x값 바탕으로 y18 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.concat([X_test.reset_index(drop=True), pred_out.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['Y18']\n",
    "y = y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20)                1760      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,781\n",
      "Trainable params: 1,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Sequential() # Sequeatial Model\n",
    "model.add(LSTM(20, input_shape=(X.shape[1], 1))) # (timestep, feature)\n",
    "model.add(Dense(1)) # output = 1\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "X = X.reshape(X.shape[0], 58, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "432/432 [==============================] - ETA: 6s - loss: 820.503 - ETA: 1s - loss: 781.329 - ETA: 0s - loss: 757.855 - ETA: 0s - loss: 758.613 - ETA: 0s - loss: 761.886 - 1s 2ms/step - loss: 763.3829\n",
      "Epoch 2/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 827.989 - ETA: 0s - loss: 759.091 - ETA: 0s - loss: 746.353 - ETA: 0s - loss: 741.330 - ETA: 0s - loss: 738.588 - 0s 776us/step - loss: 742.0738\n",
      "Epoch 3/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 748.272 - ETA: 0s - loss: 745.322 - ETA: 0s - loss: 728.012 - ETA: 0s - loss: 703.550 - ETA: 0s - loss: 711.573 - 0s 785us/step - loss: 721.1557\n",
      "Epoch 4/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 694.113 - ETA: 0s - loss: 746.601 - ETA: 0s - loss: 696.153 - ETA: 0s - loss: 696.022 - ETA: 0s - loss: 697.424 - 0s 771us/step - loss: 696.8721\n",
      "Epoch 5/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 666.209 - ETA: 0s - loss: 663.031 - ETA: 0s - loss: 615.311 - ETA: 0s - loss: 652.961 - ETA: 0s - loss: 671.190 - ETA: 0s - loss: 668.769 - 0s 935us/step - loss: 667.6213\n",
      "Epoch 6/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 634.233 - ETA: 0s - loss: 647.557 - ETA: 0s - loss: 637.347 - ETA: 0s - loss: 640.407 - ETA: 0s - loss: 630.846 - ETA: 0s - loss: 638.487 - 0s 838us/step - loss: 635.5111\n",
      "Epoch 7/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 753.751 - ETA: 0s - loss: 660.826 - ETA: 0s - loss: 633.817 - ETA: 0s - loss: 635.570 - ETA: 0s - loss: 604.384 - ETA: 0s - loss: 603.385 - 0s 826us/step - loss: 603.3305\n",
      "Epoch 8/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 607.376 - ETA: 0s - loss: 567.640 - ETA: 0s - loss: 564.827 - ETA: 0s - loss: 571.977 - ETA: 0s - loss: 569.997 - ETA: 0s - loss: 583.242 - 0s 850us/step - loss: 574.5123\n",
      "Epoch 9/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 539.599 - ETA: 0s - loss: 539.127 - ETA: 0s - loss: 557.387 - ETA: 0s - loss: 551.539 - ETA: 0s - loss: 555.786 - ETA: 0s - loss: 552.102 - 0s 866us/step - loss: 551.8897\n",
      "Epoch 10/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 514.999 - ETA: 0s - loss: 509.272 - ETA: 0s - loss: 541.821 - ETA: 0s - loss: 531.731 - ETA: 0s - loss: 519.585 - ETA: 0s - loss: 531.992 - 0s 880us/step - loss: 534.6499\n",
      "Epoch 11/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 563.105 - ETA: 0s - loss: 491.802 - ETA: 0s - loss: 517.260 - ETA: 0s - loss: 523.654 - ETA: 0s - loss: 530.246 - ETA: 0s - loss: 519.779 - 0s 868us/step - loss: 519.4708\n",
      "Epoch 12/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 461.676 - ETA: 0s - loss: 511.554 - ETA: 0s - loss: 494.808 - ETA: 0s - loss: 514.404 - ETA: 0s - loss: 512.808 - 0s 778us/step - loss: 502.4566\n",
      "Epoch 13/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 464.308 - ETA: 0s - loss: 482.653 - ETA: 0s - loss: 503.974 - ETA: 0s - loss: 498.172 - ETA: 0s - loss: 483.486 - ETA: 0s - loss: 484.742 - 0s 880us/step - loss: 484.0626\n",
      "Epoch 14/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 616.656 - ETA: 0s - loss: 529.776 - ETA: 0s - loss: 520.700 - ETA: 0s - loss: 506.668 - ETA: 0s - loss: 493.582 - ETA: 0s - loss: 482.129 - 0s 903us/step - loss: 465.2600\n",
      "Epoch 15/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 452.094 - ETA: 0s - loss: 433.343 - ETA: 0s - loss: 465.338 - ETA: 0s - loss: 453.274 - ETA: 0s - loss: 440.998 - ETA: 0s - loss: 453.737 - ETA: 0s - loss: 447.355 - 0s 947us/step - loss: 445.3665\n",
      "Epoch 16/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 489.045 - ETA: 0s - loss: 499.697 - ETA: 0s - loss: 474.874 - ETA: 0s - loss: 453.697 - ETA: 0s - loss: 444.840 - ETA: 0s - loss: 441.870 - ETA: 0s - loss: 432.143 - 0s 928us/step - loss: 432.9296\n",
      "Epoch 17/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 409.700 - ETA: 0s - loss: 384.103 - ETA: 0s - loss: 423.410 - ETA: 0s - loss: 421.825 - ETA: 0s - loss: 420.528 - 0s 796us/step - loss: 422.2116\n",
      "Epoch 18/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 410.424 - ETA: 0s - loss: 442.207 - ETA: 0s - loss: 417.154 - ETA: 0s - loss: 405.193 - ETA: 0s - loss: 418.292 - ETA: 0s - loss: 416.629 - 0s 852us/step - loss: 412.3594\n",
      "Epoch 19/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 413.320 - ETA: 0s - loss: 421.337 - ETA: 0s - loss: 420.056 - ETA: 0s - loss: 410.646 - ETA: 0s - loss: 398.729 - 0s 817us/step - loss: 402.9766\n",
      "Epoch 20/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 434.976 - ETA: 0s - loss: 404.529 - ETA: 0s - loss: 414.252 - ETA: 0s - loss: 406.113 - ETA: 0s - loss: 404.821 - ETA: 0s - loss: 394.262 - 0s 866us/step - loss: 394.0236\n",
      "Epoch 21/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 395.195 - ETA: 0s - loss: 396.608 - ETA: 0s - loss: 403.861 - ETA: 0s - loss: 395.551 - ETA: 0s - loss: 389.518 - ETA: 0s - loss: 385.889 - 0s 820us/step - loss: 385.3708\n",
      "Epoch 22/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 392.691 - ETA: 0s - loss: 413.521 - ETA: 0s - loss: 374.898 - ETA: 0s - loss: 383.138 - ETA: 0s - loss: 387.059 - ETA: 0s - loss: 378.283 - 0s 958us/step - loss: 377.0526\n",
      "Epoch 23/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 359.915 - ETA: 0s - loss: 378.049 - ETA: 0s - loss: 386.463 - ETA: 0s - loss: 370.302 - ETA: 0s - loss: 374.850 - ETA: 0s - loss: 373.302 - ETA: 0s - loss: 368.993 - 0s 947us/step - loss: 368.9843\n",
      "Epoch 24/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 334.804 - ETA: 0s - loss: 357.822 - ETA: 0s - loss: 361.375 - ETA: 0s - loss: 359.511 - ETA: 0s - loss: 361.740 - ETA: 0s - loss: 363.477 - 0s 831us/step - loss: 361.1432\n",
      "Epoch 25/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 362.272 - ETA: 0s - loss: 349.946 - ETA: 0s - loss: 353.503 - ETA: 0s - loss: 346.004 - ETA: 0s - loss: 354.587 - ETA: 0s - loss: 351.934 - ETA: 0s - loss: 352.347 - 0s 896us/step - loss: 353.5462\n",
      "Epoch 26/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 313.324 - ETA: 0s - loss: 332.009 - ETA: 0s - loss: 363.135 - ETA: 0s - loss: 355.906 - ETA: 0s - loss: 349.268 - 0s 803us/step - loss: 346.1377\n",
      "Epoch 27/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 274.868 - ETA: 0s - loss: 311.007 - ETA: 0s - loss: 330.236 - ETA: 0s - loss: 338.812 - ETA: 0s - loss: 339.643 - 0s 734us/step - loss: 338.8791\n",
      "Epoch 28/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 315.577 - ETA: 0s - loss: 349.955 - ETA: 0s - loss: 343.595 - ETA: 0s - loss: 331.767 - ETA: 0s - loss: 327.955 - ETA: 0s - loss: 330.467 - 0s 836us/step - loss: 331.8397\n",
      "Epoch 29/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 402.469 - ETA: 0s - loss: 357.922 - ETA: 0s - loss: 361.976 - ETA: 0s - loss: 340.804 - ETA: 0s - loss: 322.968 - ETA: 0s - loss: 326.042 - 0s 850us/step - loss: 324.9574\n",
      "Epoch 30/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 280.229 - ETA: 0s - loss: 304.343 - ETA: 0s - loss: 316.996 - ETA: 0s - loss: 318.455 - ETA: 0s - loss: 320.131 - ETA: 0s - loss: 313.011 - 0s 884us/step - loss: 318.1777\n",
      "Epoch 31/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 367.194 - ETA: 0s - loss: 318.101 - ETA: 0s - loss: 312.115 - ETA: 0s - loss: 320.487 - ETA: 0s - loss: 312.664 - ETA: 0s - loss: 308.385 - ETA: 0s - loss: 315.369 - 0s 1ms/step - loss: 311.6163\n",
      "Epoch 32/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 313.094 - ETA: 0s - loss: 308.886 - ETA: 0s - loss: 309.683 - ETA: 0s - loss: 304.378 - ETA: 0s - loss: 307.807 - ETA: 0s - loss: 312.012 - ETA: 0s - loss: 307.074 - 0s 947us/step - loss: 305.1688\n",
      "Epoch 33/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 371.394 - ETA: 0s - loss: 306.487 - ETA: 0s - loss: 312.216 - ETA: 0s - loss: 309.211 - ETA: 0s - loss: 301.295 - 0s 810us/step - loss: 298.8889\n",
      "Epoch 34/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 291.092 - ETA: 0s - loss: 301.199 - ETA: 0s - loss: 300.652 - ETA: 0s - loss: 296.991 - ETA: 0s - loss: 299.991 - ETA: 0s - loss: 294.673 - ETA: 0s - loss: 291.698 - 0s 910us/step - loss: 292.7256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 275.781 - ETA: 0s - loss: 285.285 - ETA: 0s - loss: 286.946 - ETA: 0s - loss: 279.454 - ETA: 0s - loss: 285.325 - ETA: 0s - loss: 286.004 - 0s 836us/step - loss: 286.6949\n",
      "Epoch 36/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 261.155 - ETA: 0s - loss: 260.269 - ETA: 0s - loss: 278.083 - ETA: 0s - loss: 270.727 - ETA: 0s - loss: 275.638 - ETA: 0s - loss: 278.957 - 0s 836us/step - loss: 280.7760\n",
      "Epoch 37/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 300.585 - ETA: 0s - loss: 280.159 - ETA: 0s - loss: 280.186 - ETA: 0s - loss: 280.393 - ETA: 0s - loss: 286.077 - ETA: 0s - loss: 284.690 - 0s 873us/step - loss: 275.0394\n",
      "Epoch 38/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 215.380 - ETA: 0s - loss: 242.633 - ETA: 0s - loss: 274.310 - ETA: 0s - loss: 257.428 - ETA: 0s - loss: 261.456 - ETA: 0s - loss: 270.302 - 0s 831us/step - loss: 269.3068\n",
      "Epoch 39/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 247.640 - ETA: 0s - loss: 251.803 - ETA: 0s - loss: 264.084 - ETA: 0s - loss: 270.169 - ETA: 0s - loss: 274.297 - ETA: 0s - loss: 268.297 - ETA: 0s - loss: 262.434 - ETA: 0s - loss: 264.646 - 0s 1ms/step - loss: 263.8291\n",
      "Epoch 40/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 284.857 - ETA: 0s - loss: 248.691 - ETA: 0s - loss: 262.508 - ETA: 0s - loss: 261.899 - ETA: 0s - loss: 254.631 - ETA: 0s - loss: 259.954 - 0s 863us/step - loss: 258.3816\n",
      "Epoch 41/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 260.427 - ETA: 0s - loss: 267.139 - ETA: 0s - loss: 261.106 - ETA: 0s - loss: 264.710 - ETA: 0s - loss: 258.470 - ETA: 0s - loss: 257.710 - 0s 868us/step - loss: 253.0749\n",
      "Epoch 42/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 202.914 - ETA: 0s - loss: 231.332 - ETA: 0s - loss: 256.214 - ETA: 0s - loss: 255.989 - ETA: 0s - loss: 253.263 - ETA: 0s - loss: 253.713 - ETA: 0s - loss: 249.671 - 0s 960us/step - loss: 247.8334\n",
      "Epoch 43/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 227.620 - ETA: 0s - loss: 237.984 - ETA: 0s - loss: 246.453 - ETA: 0s - loss: 241.874 - ETA: 0s - loss: 242.352 - ETA: 0s - loss: 241.599 - ETA: 0s - loss: 246.332 - 0s 1ms/step - loss: 242.7348\n",
      "Epoch 44/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 271.368 - ETA: 0s - loss: 274.892 - ETA: 0s - loss: 243.173 - ETA: 0s - loss: 238.009 - ETA: 0s - loss: 235.032 - ETA: 0s - loss: 238.687 - 0s 815us/step - loss: 237.7439\n",
      "Epoch 45/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 196.089 - ETA: 0s - loss: 230.398 - ETA: 0s - loss: 232.882 - ETA: 0s - loss: 236.672 - ETA: 0s - loss: 231.054 - ETA: 0s - loss: 228.006 - 0s 884us/step - loss: 232.8313\n",
      "Epoch 46/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 235.228 - ETA: 0s - loss: 251.969 - ETA: 0s - loss: 239.408 - ETA: 0s - loss: 229.266 - ETA: 0s - loss: 224.202 - ETA: 0s - loss: 228.381 - ETA: 0s - loss: 230.777 - 0s 891us/step - loss: 228.0356\n",
      "Epoch 47/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 258.661 - ETA: 0s - loss: 250.007 - ETA: 0s - loss: 234.271 - ETA: 0s - loss: 226.790 - ETA: 0s - loss: 226.315 - ETA: 0s - loss: 225.458 - ETA: 0s - loss: 229.536 - ETA: 0s - loss: 224.634 - 1s 1ms/step - loss: 223.3293\n",
      "Epoch 48/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 216.191 - ETA: 0s - loss: 216.711 - ETA: 0s - loss: 227.249 - ETA: 0s - loss: 226.189 - ETA: 0s - loss: 221.004 - ETA: 0s - loss: 216.367 - 0s 942us/step - loss: 218.7140\n",
      "Epoch 49/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 203.678 - ETA: 0s - loss: 212.192 - ETA: 0s - loss: 206.155 - ETA: 0s - loss: 196.128 - ETA: 0s - loss: 208.293 - ETA: 0s - loss: 213.813 - 0s 884us/step - loss: 214.1799\n",
      "Epoch 50/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 173.579 - ETA: 0s - loss: 201.198 - ETA: 0s - loss: 205.342 - ETA: 0s - loss: 214.183 - ETA: 0s - loss: 208.550 - ETA: 0s - loss: 209.042 - 0s 866us/step - loss: 209.7560\n",
      "Epoch 51/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 194.456 - ETA: 0s - loss: 213.206 - ETA: 0s - loss: 207.750 - ETA: 0s - loss: 209.187 - ETA: 0s - loss: 210.034 - ETA: 0s - loss: 210.739 - ETA: 0s - loss: 203.848 - 0s 933us/step - loss: 205.3730\n",
      "Epoch 52/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 180.103 - ETA: 0s - loss: 186.935 - ETA: 0s - loss: 195.343 - ETA: 0s - loss: 205.981 - ETA: 0s - loss: 200.653 - ETA: 0s - loss: 200.517 - ETA: 0s - loss: 198.924 - 0s 951us/step - loss: 201.0735\n",
      "Epoch 53/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 192.582 - ETA: 0s - loss: 188.741 - ETA: 0s - loss: 187.102 - ETA: 0s - loss: 196.424 - ETA: 0s - loss: 196.960 - ETA: 0s - loss: 197.010 - 0s 880us/step - loss: 196.8843\n",
      "Epoch 54/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 217.632 - ETA: 0s - loss: 209.998 - ETA: 0s - loss: 198.215 - ETA: 0s - loss: 188.038 - ETA: 0s - loss: 198.291 - ETA: 0s - loss: 196.207 - ETA: 0s - loss: 196.460 - 0s 951us/step - loss: 192.8179\n",
      "Epoch 55/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 219.647 - ETA: 0s - loss: 198.588 - ETA: 0s - loss: 198.238 - ETA: 0s - loss: 207.714 - ETA: 0s - loss: 201.368 - ETA: 0s - loss: 194.197 - ETA: 0s - loss: 189.840 - ETA: 0s - loss: 189.917 - 0s 1ms/step - loss: 188.8202\n",
      "Epoch 56/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 173.062 - ETA: 0s - loss: 181.424 - ETA: 0s - loss: 169.418 - ETA: 0s - loss: 179.673 - ETA: 0s - loss: 181.511 - ETA: 0s - loss: 187.397 - ETA: 0s - loss: 184.748 - 0s 930us/step - loss: 184.8864\n",
      "Epoch 57/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 203.767 - ETA: 0s - loss: 187.377 - ETA: 0s - loss: 188.296 - ETA: 0s - loss: 190.731 - ETA: 0s - loss: 187.219 - ETA: 0s - loss: 180.827 - 0s 900us/step - loss: 181.0960\n",
      "Epoch 58/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 181.833 - ETA: 0s - loss: 180.625 - ETA: 0s - loss: 188.378 - ETA: 0s - loss: 177.573 - ETA: 0s - loss: 174.805 - ETA: 0s - loss: 176.433 - 0s 898us/step - loss: 177.3441\n",
      "Epoch 59/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 169.036 - ETA: 0s - loss: 153.461 - ETA: 0s - loss: 170.632 - ETA: 0s - loss: 171.849 - ETA: 0s - loss: 172.960 - ETA: 0s - loss: 173.308 - 0s 847us/step - loss: 173.6799\n",
      "Epoch 60/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 158.954 - ETA: 0s - loss: 165.366 - ETA: 0s - loss: 160.558 - ETA: 0s - loss: 166.848 - ETA: 0s - loss: 171.560 - ETA: 0s - loss: 172.139 - ETA: 0s - loss: 171.679 - 0s 977us/step - loss: 170.0932\n",
      "Epoch 61/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 184.223 - ETA: 0s - loss: 177.375 - ETA: 0s - loss: 169.981 - ETA: 0s - loss: 172.204 - ETA: 0s - loss: 167.190 - ETA: 0s - loss: 166.664 - 0s 833us/step - loss: 166.5884\n",
      "Epoch 62/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 112.959 - ETA: 0s - loss: 138.102 - ETA: 0s - loss: 138.595 - ETA: 0s - loss: 158.616 - ETA: 0s - loss: 157.971 - ETA: 0s - loss: 160.862 - 0s 843us/step - loss: 163.1115\n",
      "Epoch 63/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 193.422 - ETA: 0s - loss: 156.484 - ETA: 0s - loss: 174.330 - ETA: 0s - loss: 172.018 - ETA: 0s - loss: 161.889 - ETA: 0s - loss: 162.184 - ETA: 0s - loss: 158.141 - 0s 1ms/step - loss: 159.7968\n",
      "Epoch 64/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 107.278 - ETA: 0s - loss: 147.533 - ETA: 0s - loss: 163.942 - ETA: 0s - loss: 158.706 - ETA: 0s - loss: 157.046 - ETA: 0s - loss: 157.842 - ETA: 0s - loss: 156.434 - 0s 880us/step - loss: 156.4857\n",
      "Epoch 65/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 141.162 - ETA: 0s - loss: 173.926 - ETA: 0s - loss: 156.418 - ETA: 0s - loss: 152.875 - ETA: 0s - loss: 151.514 - ETA: 0s - loss: 153.513 - ETA: 0s - loss: 158.069 - 0s 956us/step - loss: 153.2572\n",
      "Epoch 66/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 161.027 - ETA: 0s - loss: 167.529 - ETA: 0s - loss: 160.982 - ETA: 0s - loss: 156.782 - ETA: 0s - loss: 147.081 - ETA: 0s - loss: 149.274 - 0s 845us/step - loss: 150.1180\n",
      "Epoch 67/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 139.046 - ETA: 0s - loss: 156.968 - ETA: 0s - loss: 157.189 - ETA: 0s - loss: 151.646 - ETA: 0s - loss: 149.477 - 0s 820us/step - loss: 147.0371\n",
      "Epoch 68/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 160.993 - ETA: 0s - loss: 127.724 - ETA: 0s - loss: 133.364 - ETA: 0s - loss: 145.750 - ETA: 0s - loss: 145.448 - ETA: 0s - loss: 140.478 - ETA: 0s - loss: 145.864 - 0s 898us/step - loss: 144.0114\n",
      "Epoch 69/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 137.424 - ETA: 0s - loss: 151.384 - ETA: 0s - loss: 155.593 - ETA: 0s - loss: 151.839 - ETA: 0s - loss: 145.628 - 0s 803us/step - loss: 141.1241\n",
      "Epoch 70/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 131.134 - ETA: 0s - loss: 134.626 - ETA: 0s - loss: 126.434 - ETA: 0s - loss: 131.790 - ETA: 0s - loss: 141.920 - ETA: 0s - loss: 135.764 - ETA: 0s - loss: 139.202 - 0s 947us/step - loss: 138.2017\n",
      "Epoch 71/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 139.758 - ETA: 0s - loss: 150.090 - ETA: 0s - loss: 142.976 - ETA: 0s - loss: 138.834 - ETA: 0s - loss: 145.096 - ETA: 0s - loss: 144.301 - ETA: 0s - loss: 134.946 - 0s 993us/step - loss: 135.4259\n",
      "Epoch 72/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 149.117 - ETA: 0s - loss: 145.127 - ETA: 0s - loss: 148.015 - ETA: 0s - loss: 141.072 - ETA: 0s - loss: 132.748 - ETA: 0s - loss: 131.903 - 0s 845us/step - loss: 132.6553\n",
      "Epoch 73/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 83.57 - ETA: 0s - loss: 119.433 - ETA: 0s - loss: 139.038 - ETA: 0s - loss: 134.208 - ETA: 0s - loss: 136.910 - ETA: 0s - loss: 132.441 - 0s 880us/step - loss: 129.9508\n",
      "Epoch 74/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 104.161 - ETA: 0s - loss: 112.070 - ETA: 0s - loss: 117.095 - ETA: 0s - loss: 125.797 - ETA: 0s - loss: 129.538 - ETA: 0s - loss: 126.355 - ETA: 0s - loss: 124.865 - 0s 953us/step - loss: 127.2913\n",
      "Epoch 75/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 88.04 - ETA: 0s - loss: 118.644 - ETA: 0s - loss: 118.196 - ETA: 0s - loss: 123.008 - ETA: 0s - loss: 122.090 - ETA: 0s - loss: 122.586 - 0s 847us/step - loss: 124.7031\n",
      "Epoch 76/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 110.219 - ETA: 0s - loss: 112.263 - ETA: 0s - loss: 122.207 - ETA: 0s - loss: 120.573 - ETA: 0s - loss: 118.921 - ETA: 0s - loss: 122.165 - 0s 887us/step - loss: 122.2017\n",
      "Epoch 77/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 148.136 - ETA: 0s - loss: 117.964 - ETA: 0s - loss: 122.151 - ETA: 0s - loss: 119.113 - ETA: 0s - loss: 118.252 - ETA: 0s - loss: 121.360 - ETA: 0s - loss: 119.892 - 0s 914us/step - loss: 119.7570\n",
      "Epoch 78/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 141.172 - ETA: 0s - loss: 126.444 - ETA: 0s - loss: 122.449 - ETA: 0s - loss: 114.115 - ETA: 0s - loss: 114.017 - ETA: 0s - loss: 115.757 - 0s 863us/step - loss: 117.3545\n",
      "Epoch 79/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 102.595 - ETA: 0s - loss: 107.113 - ETA: 0s - loss: 108.310 - ETA: 0s - loss: 110.531 - ETA: 0s - loss: 109.992 - ETA: 0s - loss: 114.464 - ETA: 0s - loss: 115.695 - 0s 1ms/step - loss: 114.9940\n",
      "Epoch 80/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 92.08 - ETA: 0s - loss: 119.152 - ETA: 0s - loss: 116.573 - ETA: 0s - loss: 118.329 - ETA: 0s - loss: 114.837 - ETA: 0s - loss: 114.728 - 0s 845us/step - loss: 112.7181\n",
      "Epoch 81/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 136.732 - ETA: 0s - loss: 125.082 - ETA: 0s - loss: 111.694 - ETA: 0s - loss: 108.855 - ETA: 0s - loss: 110.925 - ETA: 0s - loss: 111.636 - ETA: 0s - loss: 111.567 - 0s 944us/step - loss: 110.4878\n",
      "Epoch 82/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 130.985 - ETA: 0s - loss: 124.482 - ETA: 0s - loss: 113.344 - ETA: 0s - loss: 119.882 - ETA: 0s - loss: 113.905 - ETA: 0s - loss: 106.402 - 0s 852us/step - loss: 108.3357\n",
      "Epoch 83/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 82.56 - ETA: 0s - loss: 112.111 - ETA: 0s - loss: 103.001 - ETA: 0s - loss: 106.284 - ETA: 0s - loss: 108.037 - ETA: 0s - loss: 106.635 - ETA: 0s - loss: 107.941 - 0s 949us/step - loss: 106.2016\n",
      "Epoch 84/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 128.731 - ETA: 0s - loss: 90.557 - ETA: 0s - loss: 100.223 - ETA: 0s - loss: 108.615 - ETA: 0s - loss: 102.705 - ETA: 0s - loss: 102.832 - 0s 852us/step - loss: 104.1475\n",
      "Epoch 85/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 107.876 - ETA: 0s - loss: 110.906 - ETA: 0s - loss: 108.713 - ETA: 0s - loss: 100.678 - ETA: 0s - loss: 104.682 - ETA: 0s - loss: 104.254 - ETA: 0s - loss: 102.343 - 0s 935us/step - loss: 102.1463\n",
      "Epoch 86/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 56.30 - ETA: 0s - loss: 99.35 - ETA: 0s - loss: 93.98 - ETA: 0s - loss: 95.64 - ETA: 0s - loss: 94.97 - ETA: 0s - loss: 101.459 - 0s 820us/step - loss: 100.1647\n",
      "Epoch 87/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 88.42 - ETA: 0s - loss: 115.289 - ETA: 0s - loss: 113.519 - ETA: 0s - loss: 104.351 - ETA: 0s - loss: 104.629 - ETA: 0s - loss: 106.676 - ETA: 0s - loss: 106.810 - ETA: 0s - loss: 98.787 - 0s 1ms/step - loss: 98.3006\n",
      "Epoch 88/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 129.148 - ETA: 0s - loss: 98.882 - ETA: 0s - loss: 93.19 - ETA: 0s - loss: 95.89 - ETA: 0s - loss: 99.23 - ETA: 0s - loss: 97.90 - 0s 845us/step - loss: 96.3872\n",
      "Epoch 89/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 109.873 - ETA: 0s - loss: 90.824 - ETA: 0s - loss: 92.54 - ETA: 0s - loss: 92.50 - ETA: 0s - loss: 96.47 - ETA: 0s - loss: 96.12 - 0s 838us/step - loss: 94.5692\n",
      "Epoch 90/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 71.06 - ETA: 0s - loss: 87.52 - ETA: 0s - loss: 89.25 - ETA: 0s - loss: 87.69 - ETA: 0s - loss: 82.01 - ETA: 0s - loss: 92.24 - 0s 845us/step - loss: 92.7706\n",
      "Epoch 91/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 76.31 - ETA: 0s - loss: 88.93 - ETA: 0s - loss: 85.23 - ETA: 0s - loss: 90.35 - ETA: 0s - loss: 92.13 - ETA: 0s - loss: 91.54 - ETA: 0s - loss: 88.62 - 0s 893us/step - loss: 91.0730\n",
      "Epoch 92/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 113.144 - ETA: 0s - loss: 83.165 - ETA: 0s - loss: 81.22 - ETA: 0s - loss: 88.78 - ETA: 0s - loss: 89.72 - ETA: 0s - loss: 92.88 - 0s 859us/step - loss: 89.3654\n",
      "Epoch 93/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 103.852 - ETA: 0s - loss: 82.960 - ETA: 0s - loss: 87.44 - ETA: 0s - loss: 90.16 - ETA: 0s - loss: 92.43 - ETA: 0s - loss: 88.17 - ETA: 0s - loss: 88.66 - 0s 947us/step - loss: 87.6993\n",
      "Epoch 94/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 98.83 - ETA: 0s - loss: 81.36 - ETA: 0s - loss: 91.67 - ETA: 0s - loss: 90.99 - ETA: 0s - loss: 86.01 - ETA: 0s - loss: 86.69 - ETA: 0s - loss: 86.21 - 0s 930us/step - loss: 86.0978\n",
      "Epoch 95/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 66.84 - ETA: 0s - loss: 85.48 - ETA: 0s - loss: 95.27 - ETA: 0s - loss: 88.70 - ETA: 0s - loss: 87.88 - ETA: 0s - loss: 84.16 - ETA: 0s - loss: 84.31 - 0s 1ms/step - loss: 84.5442\n",
      "Epoch 96/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 95.17 - ETA: 0s - loss: 85.74 - ETA: 0s - loss: 87.26 - ETA: 0s - loss: 81.40 - ETA: 0s - loss: 79.34 - ETA: 0s - loss: 82.14 - ETA: 0s - loss: 84.25 - 0s 905us/step - loss: 83.0079\n",
      "Epoch 97/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 79.04 - ETA: 0s - loss: 91.36 - ETA: 0s - loss: 82.40 - ETA: 0s - loss: 82.80 - ETA: 0s - loss: 85.88 - ETA: 0s - loss: 81.98 - 0s 840us/step - loss: 81.5563\n",
      "Epoch 98/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - ETA: 0s - loss: 62.13 - ETA: 0s - loss: 73.21 - ETA: 0s - loss: 79.88 - ETA: 0s - loss: 80.72 - ETA: 0s - loss: 81.82 - ETA: 0s - loss: 77.80 - ETA: 0s - loss: 79.10 - 0s 926us/step - loss: 80.1110\n",
      "Epoch 99/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 44.44 - ETA: 0s - loss: 80.08 - ETA: 0s - loss: 79.88 - ETA: 0s - loss: 81.81 - ETA: 0s - loss: 81.15 - ETA: 0s - loss: 80.33 - ETA: 0s - loss: 79.23 - 0s 919us/step - loss: 78.7112\n",
      "Epoch 100/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 115.228 - ETA: 0s - loss: 91.166 - ETA: 0s - loss: 81.59 - ETA: 0s - loss: 76.22 - ETA: 0s - loss: 76.80 - 0s 787us/step - loss: 77.3394\n",
      "Epoch 101/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 68.03 - ETA: 0s - loss: 61.08 - ETA: 0s - loss: 66.92 - ETA: 0s - loss: 71.79 - ETA: 0s - loss: 77.56 - 0s 790us/step - loss: 76.0136\n",
      "Epoch 102/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 47.18 - ETA: 0s - loss: 67.06 - ETA: 0s - loss: 68.25 - ETA: 0s - loss: 70.54 - ETA: 0s - loss: 74.00 - 0s 787us/step - loss: 74.7331\n",
      "Epoch 103/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 78.44 - ETA: 0s - loss: 85.44 - ETA: 0s - loss: 80.78 - ETA: 0s - loss: 73.27 - ETA: 0s - loss: 73.27 - 0s 815us/step - loss: 73.5112\n",
      "Epoch 104/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 71.54 - ETA: 0s - loss: 79.12 - ETA: 0s - loss: 80.86 - ETA: 0s - loss: 80.85 - ETA: 0s - loss: 80.13 - ETA: 0s - loss: 77.02 - ETA: 0s - loss: 74.47 - 0s 1ms/step - loss: 72.2928\n",
      "Epoch 105/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 52.47 - ETA: 0s - loss: 66.85 - ETA: 0s - loss: 68.55 - ETA: 0s - loss: 68.53 - ETA: 0s - loss: 67.80 - ETA: 0s - loss: 71.62 - ETA: 0s - loss: 69.05 - 0s 903us/step - loss: 71.0912\n",
      "Epoch 106/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 87.66 - ETA: 0s - loss: 75.72 - ETA: 0s - loss: 68.24 - ETA: 0s - loss: 68.85 - ETA: 0s - loss: 70.57 - 0s 817us/step - loss: 69.9578\n",
      "Epoch 107/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 72.11 - ETA: 0s - loss: 64.32 - ETA: 0s - loss: 62.32 - ETA: 0s - loss: 62.13 - ETA: 0s - loss: 58.73 - ETA: 0s - loss: 58.42 - ETA: 0s - loss: 67.04 - 0s 907us/step - loss: 68.8195\n",
      "Epoch 108/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 71.53 - ETA: 0s - loss: 61.09 - ETA: 0s - loss: 62.46 - ETA: 0s - loss: 68.84 - ETA: 0s - loss: 68.12 - ETA: 0s - loss: 68.57 - 0s 900us/step - loss: 67.7669\n",
      "Epoch 109/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 72.40 - ETA: 0s - loss: 62.93 - ETA: 0s - loss: 63.86 - ETA: 0s - loss: 67.40 - ETA: 0s - loss: 71.57 - ETA: 0s - loss: 67.45 - 0s 843us/step - loss: 66.6936\n",
      "Epoch 110/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 90.58 - ETA: 0s - loss: 77.39 - ETA: 0s - loss: 76.55 - ETA: 0s - loss: 70.95 - ETA: 0s - loss: 67.39 - 0s 790us/step - loss: 65.6928\n",
      "Epoch 111/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 68.24 - ETA: 0s - loss: 63.61 - ETA: 0s - loss: 63.99 - ETA: 0s - loss: 63.57 - ETA: 0s - loss: 64.37 - ETA: 0s - loss: 65.19 - 0s 829us/step - loss: 64.6880\n",
      "Epoch 112/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 42.19 - ETA: 0s - loss: 54.79 - ETA: 0s - loss: 63.05 - ETA: 0s - loss: 68.93 - ETA: 0s - loss: 66.22 - ETA: 0s - loss: 64.66 - 0s 947us/step - loss: 63.7541\n",
      "Epoch 113/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 78.75 - ETA: 0s - loss: 61.42 - ETA: 0s - loss: 65.23 - ETA: 0s - loss: 58.58 - ETA: 0s - loss: 60.83 - ETA: 0s - loss: 64.94 - 0s 847us/step - loss: 62.8340\n",
      "Epoch 114/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 76.02 - ETA: 0s - loss: 73.43 - ETA: 0s - loss: 66.50 - ETA: 0s - loss: 62.54 - ETA: 0s - loss: 64.24 - ETA: 0s - loss: 63.92 - 0s 850us/step - loss: 61.9473\n",
      "Epoch 115/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 81.39 - ETA: 0s - loss: 69.56 - ETA: 0s - loss: 60.85 - ETA: 0s - loss: 61.04 - ETA: 0s - loss: 59.93 - 0s 822us/step - loss: 61.0804\n",
      "Epoch 116/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 73.01 - ETA: 0s - loss: 54.47 - ETA: 0s - loss: 59.39 - ETA: 0s - loss: 62.39 - ETA: 0s - loss: 61.07 - ETA: 0s - loss: 60.16 - 0s 953us/step - loss: 60.2677\n",
      "Epoch 117/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 45.51 - ETA: 0s - loss: 61.50 - ETA: 0s - loss: 62.18 - ETA: 0s - loss: 60.15 - ETA: 0s - loss: 57.14 - ETA: 0s - loss: 60.11 - 0s 840us/step - loss: 59.4439\n",
      "Epoch 118/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 28.11 - ETA: 0s - loss: 50.84 - ETA: 0s - loss: 45.44 - ETA: 0s - loss: 50.82 - ETA: 0s - loss: 58.76 - ETA: 0s - loss: 59.63 - 0s 877us/step - loss: 58.6559\n",
      "Epoch 119/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 63.00 - ETA: 0s - loss: 71.99 - ETA: 0s - loss: 64.61 - ETA: 0s - loss: 58.95 - ETA: 0s - loss: 58.32 - ETA: 0s - loss: 57.61 - ETA: 0s - loss: 55.72 - 0s 912us/step - loss: 57.8930\n",
      "Epoch 120/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 68.63 - ETA: 0s - loss: 58.72 - ETA: 0s - loss: 59.35 - ETA: 0s - loss: 59.10 - ETA: 0s - loss: 57.41 - ETA: 0s - loss: 56.83 - ETA: 0s - loss: 57.96 - 0s 1ms/step - loss: 57.1381\n",
      "Epoch 121/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 60.67 - ETA: 0s - loss: 55.44 - ETA: 0s - loss: 58.20 - ETA: 0s - loss: 59.00 - ETA: 0s - loss: 55.20 - ETA: 0s - loss: 56.21 - 0s 857us/step - loss: 56.4011\n",
      "Epoch 122/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 62.96 - ETA: 0s - loss: 55.15 - ETA: 0s - loss: 54.05 - ETA: 0s - loss: 52.95 - ETA: 0s - loss: 52.56 - ETA: 0s - loss: 53.80 - 0s 875us/step - loss: 55.7061\n",
      "Epoch 123/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 44.98 - ETA: 0s - loss: 55.03 - ETA: 0s - loss: 52.79 - ETA: 0s - loss: 53.72 - ETA: 0s - loss: 52.58 - ETA: 0s - loss: 52.55 - 0s 863us/step - loss: 55.0256\n",
      "Epoch 124/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 32.68 - ETA: 0s - loss: 42.68 - ETA: 0s - loss: 52.66 - ETA: 0s - loss: 48.94 - ETA: 0s - loss: 47.33 - ETA: 0s - loss: 49.46 - ETA: 0s - loss: 55.30 - 0s 930us/step - loss: 54.3779\n",
      "Epoch 125/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 75.03 - ETA: 0s - loss: 61.90 - ETA: 0s - loss: 52.98 - ETA: 0s - loss: 52.40 - ETA: 0s - loss: 51.43 - ETA: 0s - loss: 52.50 - 0s 875us/step - loss: 53.7586\n",
      "Epoch 126/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 87.23 - ETA: 0s - loss: 59.75 - ETA: 0s - loss: 58.46 - ETA: 0s - loss: 57.88 - ETA: 0s - loss: 57.20 - ETA: 0s - loss: 51.63 - 0s 889us/step - loss: 53.1740\n",
      "Epoch 127/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 41.74 - ETA: 0s - loss: 53.22 - ETA: 0s - loss: 51.45 - ETA: 0s - loss: 51.68 - ETA: 0s - loss: 52.70 - ETA: 0s - loss: 53.38 - ETA: 0s - loss: 52.06 - 0s 907us/step - loss: 52.5844\n",
      "Epoch 128/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 45.00 - ETA: 0s - loss: 57.14 - ETA: 0s - loss: 54.09 - ETA: 0s - loss: 53.27 - ETA: 0s - loss: 52.91 - ETA: 0s - loss: 52.22 - 0s 986us/step - loss: 52.0181\n",
      "Epoch 129/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 47.96 - ETA: 0s - loss: 60.67 - ETA: 0s - loss: 60.27 - ETA: 0s - loss: 56.61 - ETA: 0s - loss: 54.83 - ETA: 0s - loss: 53.39 - 0s 910us/step - loss: 51.4669\n",
      "Epoch 130/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 65.73 - ETA: 0s - loss: 56.55 - ETA: 0s - loss: 50.03 - ETA: 0s - loss: 52.40 - ETA: 0s - loss: 51.02 - ETA: 0s - loss: 50.75 - 0s 887us/step - loss: 50.9369\n",
      "Epoch 131/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 45.41 - ETA: 0s - loss: 44.21 - ETA: 0s - loss: 46.82 - ETA: 0s - loss: 51.96 - ETA: 0s - loss: 53.29 - ETA: 0s - loss: 51.75 - ETA: 0s - loss: 51.08 - 0s 880us/step - loss: 50.4488\n",
      "Epoch 132/1000\n",
      "432/432 [==============================] - ETA: 1s - loss: 43.99 - ETA: 0s - loss: 53.67 - ETA: 0s - loss: 55.28 - ETA: 0s - loss: 54.87 - ETA: 0s - loss: 54.59 - ETA: 0s - loss: 52.68 - ETA: 0s - loss: 50.52 - 1s 1ms/step - loss: 49.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 31.88 - ETA: 0s - loss: 55.53 - ETA: 0s - loss: 55.14 - ETA: 0s - loss: 48.45 - ETA: 0s - loss: 47.91 - ETA: 0s - loss: 51.48 - 0s 875us/step - loss: 49.5165\n",
      "Epoch 134/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 58.99 - ETA: 0s - loss: 46.37 - ETA: 0s - loss: 50.19 - ETA: 0s - loss: 48.79 - ETA: 0s - loss: 48.77 - ETA: 0s - loss: 48.94 - 0s 866us/step - loss: 49.0685\n",
      "Epoch 135/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 32.80 - ETA: 0s - loss: 41.80 - ETA: 0s - loss: 47.18 - ETA: 0s - loss: 47.36 - ETA: 0s - loss: 49.53 - ETA: 0s - loss: 46.74 - 0s 847us/step - loss: 48.6375\n",
      "Epoch 136/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 34.61 - ETA: 0s - loss: 43.24 - ETA: 0s - loss: 41.74 - ETA: 0s - loss: 44.49 - ETA: 0s - loss: 45.99 - ETA: 0s - loss: 46.52 - ETA: 0s - loss: 45.38 - ETA: 0s - loss: 47.44 - 1s 1ms/step - loss: 48.2105\n",
      "Epoch 137/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 52.20 - ETA: 0s - loss: 55.06 - ETA: 0s - loss: 50.09 - ETA: 0s - loss: 48.30 - ETA: 0s - loss: 49.69 - ETA: 0s - loss: 47.94 - 0s 847us/step - loss: 47.8102\n",
      "Epoch 138/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 41.67 - ETA: 0s - loss: 45.21 - ETA: 0s - loss: 46.25 - ETA: 0s - loss: 45.45 - ETA: 0s - loss: 45.24 - ETA: 0s - loss: 45.69 - 0s 854us/step - loss: 47.4116\n",
      "Epoch 139/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 60.16 - ETA: 0s - loss: 46.12 - ETA: 0s - loss: 42.62 - ETA: 0s - loss: 44.92 - ETA: 0s - loss: 42.43 - ETA: 0s - loss: 45.55 - 0s 843us/step - loss: 47.0316\n",
      "Epoch 140/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 32.79 - ETA: 0s - loss: 43.15 - ETA: 0s - loss: 43.03 - ETA: 0s - loss: 43.26 - ETA: 0s - loss: 43.69 - 0s 799us/step - loss: 46.6712\n",
      "Epoch 141/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 42.59 - ETA: 0s - loss: 51.41 - ETA: 0s - loss: 51.02 - ETA: 0s - loss: 52.16 - ETA: 0s - loss: 49.40 - ETA: 0s - loss: 46.14 - 0s 833us/step - loss: 46.3437\n",
      "Epoch 142/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 77.70 - ETA: 0s - loss: 59.00 - ETA: 0s - loss: 46.95 - ETA: 0s - loss: 45.21 - ETA: 0s - loss: 46.32 - ETA: 0s - loss: 45.84 - 0s 845us/step - loss: 46.0105\n",
      "Epoch 143/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 61.25 - ETA: 0s - loss: 45.90 - ETA: 0s - loss: 41.52 - ETA: 0s - loss: 41.60 - ETA: 0s - loss: 42.42 - ETA: 0s - loss: 46.00 - 0s 910us/step - loss: 45.6882\n",
      "Epoch 144/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 23.41 - ETA: 0s - loss: 27.91 - ETA: 0s - loss: 34.18 - ETA: 0s - loss: 38.27 - ETA: 0s - loss: 43.31 - ETA: 0s - loss: 46.27 - ETA: 0s - loss: 46.18 - ETA: 0s - loss: 45.96 - 1s 1ms/step - loss: 45.4002\n",
      "Epoch 145/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 39.93 - ETA: 0s - loss: 46.77 - ETA: 0s - loss: 51.20 - ETA: 0s - loss: 50.55 - ETA: 0s - loss: 48.18 - ETA: 0s - loss: 44.92 - ETA: 0s - loss: 45.06 - ETA: 0s - loss: 45.26 - ETA: 0s - loss: 45.16 - 1s 2ms/step - loss: 45.1293\n",
      "Epoch 146/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 40.39 - ETA: 0s - loss: 48.07 - ETA: 0s - loss: 46.89 - ETA: 0s - loss: 44.92 - ETA: 0s - loss: 44.69 - ETA: 0s - loss: 44.15 - ETA: 0s - loss: 42.30 - ETA: 0s - loss: 43.46 - 1s 2ms/step - loss: 44.8388\n",
      "Epoch 147/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 54.71 - ETA: 0s - loss: 40.60 - ETA: 0s - loss: 37.91 - ETA: 0s - loss: 39.16 - ETA: 0s - loss: 40.94 - ETA: 0s - loss: 41.80 - ETA: 0s - loss: 41.65 - ETA: 0s - loss: 44.51 - ETA: 0s - loss: 44.00 - 1s 1ms/step - loss: 44.5713\n",
      "Epoch 148/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 61.03 - ETA: 0s - loss: 45.79 - ETA: 0s - loss: 46.37 - ETA: 0s - loss: 45.74 - ETA: 0s - loss: 43.65 - ETA: 0s - loss: 42.90 - ETA: 0s - loss: 44.02 - ETA: 0s - loss: 42.45 - ETA: 0s - loss: 43.58 - 1s 2ms/step - loss: 44.3034\n",
      "Epoch 149/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 20.45 - ETA: 0s - loss: 33.41 - ETA: 0s - loss: 36.58 - ETA: 0s - loss: 37.86 - ETA: 0s - loss: 43.17 - ETA: 0s - loss: 43.70 - ETA: 0s - loss: 45.92 - ETA: 0s - loss: 44.67 - 1s 1ms/step - loss: 44.0531\n",
      "Epoch 150/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 43.45 - ETA: 0s - loss: 46.94 - ETA: 0s - loss: 45.66 - ETA: 0s - loss: 44.51 - ETA: 0s - loss: 45.65 - ETA: 0s - loss: 44.53 - ETA: 0s - loss: 44.44 - ETA: 0s - loss: 43.91 - 1s 1ms/step - loss: 43.8204\n",
      "Epoch 151/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 46.74 - ETA: 0s - loss: 43.68 - ETA: 0s - loss: 45.21 - ETA: 0s - loss: 42.01 - ETA: 0s - loss: 40.15 - ETA: 0s - loss: 43.06 - ETA: 0s - loss: 41.47 - ETA: 0s - loss: 42.60 - ETA: 0s - loss: 44.06 - 1s 1ms/step - loss: 43.5882\n",
      "Epoch 152/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 34.53 - ETA: 0s - loss: 34.27 - ETA: 0s - loss: 34.14 - ETA: 0s - loss: 37.11 - ETA: 0s - loss: 40.08 - ETA: 0s - loss: 40.10 - ETA: 0s - loss: 43.32 - ETA: 0s - loss: 42.65 - 1s 1ms/step - loss: 43.3688\n",
      "Epoch 153/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 32.05 - ETA: 0s - loss: 43.31 - ETA: 0s - loss: 42.55 - ETA: 0s - loss: 42.27 - ETA: 0s - loss: 42.82 - ETA: 0s - loss: 45.34 - ETA: 0s - loss: 41.80 - ETA: 0s - loss: 42.76 - 1s 1ms/step - loss: 43.1693\n",
      "Epoch 154/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 39.22 - ETA: 0s - loss: 35.39 - ETA: 0s - loss: 37.25 - ETA: 0s - loss: 39.79 - ETA: 0s - loss: 40.57 - ETA: 0s - loss: 42.20 - ETA: 0s - loss: 41.03 - ETA: 0s - loss: 43.98 - ETA: 0s - loss: 41.93 - ETA: 0s - loss: 41.17 - ETA: 0s - loss: 41.48 - 1s 2ms/step - loss: 42.9512\n",
      "Epoch 155/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 50.27 - ETA: 0s - loss: 46.07 - ETA: 0s - loss: 45.45 - ETA: 0s - loss: 45.00 - ETA: 0s - loss: 43.37 - ETA: 0s - loss: 42.82 - ETA: 0s - loss: 42.20 - ETA: 0s - loss: 43.52 - 1s 1ms/step - loss: 42.7489\n",
      "Epoch 156/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 43.12 - ETA: 0s - loss: 44.71 - ETA: 0s - loss: 42.55 - ETA: 0s - loss: 43.19 - ETA: 0s - loss: 41.04 - ETA: 0s - loss: 43.78 - ETA: 0s - loss: 41.42 - 0s 944us/step - loss: 42.2167\n",
      "Epoch 157/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 52.94 - ETA: 0s - loss: 40.39 - ETA: 0s - loss: 41.30 - ETA: 0s - loss: 40.20 - ETA: 0s - loss: 43.39 - ETA: 0s - loss: 41.83 - ETA: 0s - loss: 42.64 - ETA: 0s - loss: 41.01 - 1s 1ms/step - loss: 41.0566\n",
      "Epoch 158/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 33.55 - ETA: 0s - loss: 37.57 - ETA: 0s - loss: 36.11 - ETA: 0s - loss: 37.91 - ETA: 0s - loss: 38.28 - ETA: 0s - loss: 38.72 - ETA: 0s - loss: 37.18 - 0s 1ms/step - loss: 40.1421\n",
      "Epoch 159/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 63.10 - ETA: 0s - loss: 52.88 - ETA: 0s - loss: 45.66 - ETA: 0s - loss: 41.25 - ETA: 0s - loss: 42.76 - ETA: 0s - loss: 40.38 - ETA: 0s - loss: 39.82 - ETA: 0s - loss: 39.33 - 1s 1ms/step - loss: 39.4287\n",
      "Epoch 160/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 58.03 - ETA: 0s - loss: 47.70 - ETA: 0s - loss: 45.20 - ETA: 0s - loss: 43.17 - ETA: 0s - loss: 40.66 - ETA: 0s - loss: 40.50 - ETA: 0s - loss: 37.97 - 1s 1ms/step - loss: 38.6919\n",
      "Epoch 161/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 26.93 - ETA: 0s - loss: 36.40 - ETA: 0s - loss: 39.86 - ETA: 0s - loss: 40.59 - ETA: 0s - loss: 39.73 - ETA: 0s - loss: 42.52 - ETA: 0s - loss: 41.54 - ETA: 0s - loss: 38.88 - ETA: 0s - loss: 38.37 - 1s 2ms/step - loss: 38.0533\n",
      "Epoch 162/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 38.43 - ETA: 0s - loss: 40.45 - ETA: 0s - loss: 41.42 - ETA: 0s - loss: 40.47 - ETA: 0s - loss: 40.93 - ETA: 0s - loss: 39.52 - ETA: 0s - loss: 37.15 - 0s 947us/step - loss: 37.4154\n",
      "Epoch 163/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 28.16 - ETA: 0s - loss: 31.42 - ETA: 0s - loss: 32.25 - ETA: 0s - loss: 38.10 - ETA: 0s - loss: 38.09 - ETA: 0s - loss: 37.06 - 0s 928us/step - loss: 36.7229\n",
      "Epoch 164/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 30.35 - ETA: 0s - loss: 24.37 - ETA: 0s - loss: 30.66 - ETA: 0s - loss: 31.12 - ETA: 0s - loss: 32.38 - ETA: 0s - loss: 33.47 - ETA: 0s - loss: 36.05 - ETA: 0s - loss: 36.47 - ETA: 0s - loss: 36.38 - 1s 1ms/step - loss: 35.9950\n",
      "Epoch 165/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 30.75 - ETA: 0s - loss: 29.44 - ETA: 0s - loss: 35.62 - ETA: 0s - loss: 32.96 - ETA: 0s - loss: 32.56 - ETA: 0s - loss: 32.92 - ETA: 0s - loss: 34.88 - 0s 1ms/step - loss: 35.0187\n",
      "Epoch 166/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 32.48 - ETA: 0s - loss: 34.93 - ETA: 0s - loss: 32.53 - ETA: 0s - loss: 36.31 - ETA: 0s - loss: 35.63 - ETA: 0s - loss: 35.65 - ETA: 0s - loss: 34.54 - ETA: 0s - loss: 33.63 - 0s 1ms/step - loss: 32.8552\n",
      "Epoch 167/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 30.34 - ETA: 0s - loss: 29.93 - ETA: 0s - loss: 35.23 - ETA: 0s - loss: 28.80 - ETA: 0s - loss: 29.16 - ETA: 0s - loss: 29.20 - ETA: 0s - loss: 29.60 - 0s 1ms/step - loss: 31.7201\n",
      "Epoch 168/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 35.88 - ETA: 0s - loss: 30.24 - ETA: 0s - loss: 34.86 - ETA: 0s - loss: 32.85 - ETA: 0s - loss: 32.57 - ETA: 0s - loss: 31.70 - ETA: 0s - loss: 31.31 - ETA: 0s - loss: 31.13 - ETA: 0s - loss: 30.75 - ETA: 0s - loss: 30.70 - ETA: 0s - loss: 31.25 - 1s 2ms/step - loss: 31.0198\n",
      "Epoch 169/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 19.93 - ETA: 0s - loss: 26.23 - ETA: 0s - loss: 30.99 - ETA: 0s - loss: 30.14 - ETA: 0s - loss: 29.01 - ETA: 0s - loss: 29.88 - ETA: 0s - loss: 30.70 - 0s 1ms/step - loss: 30.3037\n",
      "Epoch 170/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 48.74 - ETA: 0s - loss: 34.13 - ETA: 0s - loss: 33.56 - ETA: 0s - loss: 29.43 - ETA: 0s - loss: 30.00 - 0s 808us/step - loss: 29.7545\n",
      "Epoch 171/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 10.24 - ETA: 0s - loss: 18.73 - ETA: 0s - loss: 21.64 - ETA: 0s - loss: 24.44 - ETA: 0s - loss: 27.10 - 0s 783us/step - loss: 29.2151\n",
      "Epoch 172/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 37.41 - ETA: 0s - loss: 25.82 - ETA: 0s - loss: 31.23 - ETA: 0s - loss: 31.20 - ETA: 0s - loss: 29.00 - ETA: 0s - loss: 29.19 - ETA: 0s - loss: 28.58 - ETA: 0s - loss: 29.26 - 0s 1ms/step - loss: 28.7070\n",
      "Epoch 173/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 31.54 - ETA: 0s - loss: 31.56 - ETA: 0s - loss: 27.74 - ETA: 0s - loss: 26.14 - ETA: 0s - loss: 27.85 - ETA: 0s - loss: 30.30 - ETA: 0s - loss: 29.59 - ETA: 0s - loss: 29.17 - 1s 1ms/step - loss: 28.1820\n",
      "Epoch 174/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 26.80 - ETA: 0s - loss: 30.35 - ETA: 0s - loss: 27.22 - ETA: 0s - loss: 26.77 - ETA: 0s - loss: 30.53 - ETA: 0s - loss: 28.44 - ETA: 0s - loss: 27.84 - 0s 884us/step - loss: 27.7257\n",
      "Epoch 175/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 18.23 - ETA: 0s - loss: 28.85 - ETA: 0s - loss: 25.19 - ETA: 0s - loss: 26.77 - ETA: 0s - loss: 27.03 - ETA: 0s - loss: 27.04 - 0s 930us/step - loss: 27.2438\n",
      "Epoch 176/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 18.85 - ETA: 0s - loss: 27.19 - ETA: 0s - loss: 26.88 - ETA: 0s - loss: 23.73 - ETA: 0s - loss: 25.16 - ETA: 0s - loss: 26.20 - 0s 963us/step - loss: 26.7463\n",
      "Epoch 177/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 32.14 - ETA: 0s - loss: 31.28 - ETA: 0s - loss: 27.07 - ETA: 0s - loss: 28.90 - ETA: 0s - loss: 28.42 - ETA: 0s - loss: 27.01 - ETA: 0s - loss: 26.82 - 0s 1ms/step - loss: 26.2933\n",
      "Epoch 178/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 24.35 - ETA: 0s - loss: 28.14 - ETA: 0s - loss: 27.53 - ETA: 0s - loss: 26.21 - ETA: 0s - loss: 26.15 - ETA: 0s - loss: 25.01 - ETA: 0s - loss: 25.48 - 0s 944us/step - loss: 25.8629\n",
      "Epoch 179/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 20.57 - ETA: 0s - loss: 20.27 - ETA: 0s - loss: 21.56 - ETA: 0s - loss: 23.38 - ETA: 0s - loss: 24.69 - ETA: 0s - loss: 26.27 - 0s 850us/step - loss: 25.4525\n",
      "Epoch 180/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 14.95 - ETA: 0s - loss: 21.88 - ETA: 0s - loss: 25.30 - ETA: 0s - loss: 26.86 - ETA: 0s - loss: 25.57 - ETA: 0s - loss: 25.88 - 0s 1ms/step - loss: 24.9967\n",
      "Epoch 181/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 13.77 - ETA: 0s - loss: 16.89 - ETA: 0s - loss: 19.12 - ETA: 0s - loss: 19.49 - ETA: 0s - loss: 21.42 - ETA: 0s - loss: 22.74 - ETA: 0s - loss: 23.26 - 0s 1ms/step - loss: 24.6234\n",
      "Epoch 182/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 25.93 - ETA: 0s - loss: 20.22 - ETA: 0s - loss: 18.72 - ETA: 0s - loss: 24.64 - ETA: 0s - loss: 22.81 - ETA: 0s - loss: 24.04 - ETA: 0s - loss: 22.81 - ETA: 0s - loss: 23.98 - 1s 1ms/step - loss: 24.1151\n",
      "Epoch 183/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 20.88 - ETA: 0s - loss: 27.51 - ETA: 0s - loss: 24.33 - ETA: 0s - loss: 23.49 - ETA: 0s - loss: 22.41 - ETA: 0s - loss: 22.15 - ETA: 0s - loss: 23.48 - ETA: 0s - loss: 23.75 - 1s 1ms/step - loss: 23.7330\n",
      "Epoch 184/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 13.86 - ETA: 0s - loss: 19.33 - ETA: 0s - loss: 21.35 - ETA: 0s - loss: 22.67 - ETA: 0s - loss: 22.63 - ETA: 0s - loss: 21.57 - ETA: 0s - loss: 23.31 - ETA: 0s - loss: 22.87 - 0s 1ms/step - loss: 23.3218\n",
      "Epoch 185/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 23.30 - ETA: 0s - loss: 22.13 - ETA: 0s - loss: 22.27 - ETA: 0s - loss: 21.92 - ETA: 0s - loss: 21.58 - ETA: 0s - loss: 22.17 - 0s 893us/step - loss: 22.9137\n",
      "Epoch 186/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 20.11 - ETA: 0s - loss: 24.88 - ETA: 0s - loss: 22.75 - ETA: 0s - loss: 23.94 - ETA: 0s - loss: 23.48 - 0s 820us/step - loss: 22.5491\n",
      "Epoch 187/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 22.09 - ETA: 0s - loss: 17.08 - ETA: 0s - loss: 20.63 - ETA: 0s - loss: 21.36 - ETA: 0s - loss: 22.37 - ETA: 0s - loss: 22.09 - ETA: 0s - loss: 22.69 - ETA: 0s - loss: 22.40 - 1s 1ms/step - loss: 22.1490\n",
      "Epoch 188/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 40.19 - ETA: 0s - loss: 30.44 - ETA: 0s - loss: 28.65 - ETA: 0s - loss: 25.36 - ETA: 0s - loss: 24.35 - ETA: 0s - loss: 23.69 - ETA: 0s - loss: 21.98 - 0s 1ms/step - loss: 21.8638\n",
      "Epoch 189/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 19.85 - ETA: 0s - loss: 20.33 - ETA: 0s - loss: 18.39 - ETA: 0s - loss: 23.79 - ETA: 0s - loss: 22.17 - ETA: 0s - loss: 21.81 - ETA: 0s - loss: 22.10 - ETA: 0s - loss: 22.30 - 1s 1ms/step - loss: 21.6375\n",
      "Epoch 190/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 21.92 - ETA: 0s - loss: 20.55 - ETA: 0s - loss: 17.34 - ETA: 0s - loss: 21.25 - ETA: 0s - loss: 19.96 - ETA: 0s - loss: 20.63 - ETA: 0s - loss: 20.61 - 0s 898us/step - loss: 21.0966\n",
      "Epoch 191/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 20.94 - ETA: 0s - loss: 19.93 - ETA: 0s - loss: 16.88 - ETA: 0s - loss: 21.67 - ETA: 0s - loss: 21.14 - ETA: 0s - loss: 22.11 - ETA: 0s - loss: 21.20 - 0s 933us/step - loss: 20.7982\n",
      "Epoch 192/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 7.210 - ETA: 0s - loss: 17.84 - ETA: 0s - loss: 21.46 - ETA: 0s - loss: 21.65 - ETA: 0s - loss: 20.36 - ETA: 0s - loss: 20.72 - 0s 850us/step - loss: 20.4698\n",
      "Epoch 193/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 22.59 - ETA: 0s - loss: 17.77 - ETA: 0s - loss: 19.13 - ETA: 0s - loss: 19.42 - ETA: 0s - loss: 19.69 - 0s 820us/step - loss: 20.0949\n",
      "Epoch 194/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 24.93 - ETA: 0s - loss: 19.12 - ETA: 0s - loss: 20.29 - ETA: 0s - loss: 19.49 - ETA: 0s - loss: 17.95 - ETA: 0s - loss: 18.70 - ETA: 0s - loss: 18.89 - ETA: 0s - loss: 19.12 - 1s 1ms/step - loss: 19.8002\n",
      "Epoch 195/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 13.21 - ETA: 0s - loss: 18.45 - ETA: 0s - loss: 17.05 - ETA: 0s - loss: 18.23 - ETA: 0s - loss: 18.27 - ETA: 0s - loss: 19.94 - ETA: 0s - loss: 20.72 - ETA: 0s - loss: 20.05 - 1s 1ms/step - loss: 19.4584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 19.06 - ETA: 0s - loss: 21.32 - ETA: 0s - loss: 21.79 - ETA: 0s - loss: 18.73 - ETA: 0s - loss: 19.49 - ETA: 0s - loss: 19.04 - 0s 993us/step - loss: 19.1574\n",
      "Epoch 197/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 10.47 - ETA: 0s - loss: 12.39 - ETA: 0s - loss: 17.77 - ETA: 0s - loss: 18.05 - ETA: 0s - loss: 19.13 - ETA: 0s - loss: 17.93 - 0s 1ms/step - loss: 18.8591\n",
      "Epoch 198/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 28.06 - ETA: 0s - loss: 18.91 - ETA: 0s - loss: 16.34 - ETA: 0s - loss: 19.35 - ETA: 0s - loss: 18.40 - 0s 769us/step - loss: 18.5604\n",
      "Epoch 199/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 22.79 - ETA: 0s - loss: 15.94 - ETA: 0s - loss: 17.04 - ETA: 0s - loss: 17.47 - ETA: 0s - loss: 18.81 - 0s 806us/step - loss: 18.2766\n",
      "Epoch 200/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 28.92 - ETA: 0s - loss: 22.24 - ETA: 0s - loss: 19.32 - ETA: 0s - loss: 19.31 - ETA: 0s - loss: 18.19 - 0s 808us/step - loss: 18.0325\n",
      "Epoch 201/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 16.00 - ETA: 0s - loss: 20.04 - ETA: 0s - loss: 17.83 - ETA: 0s - loss: 17.09 - ETA: 0s - loss: 17.80 - 0s 799us/step - loss: 17.7335\n",
      "Epoch 202/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 14.39 - ETA: 0s - loss: 19.45 - ETA: 0s - loss: 19.79 - ETA: 0s - loss: 18.22 - ETA: 0s - loss: 18.09 - ETA: 0s - loss: 17.55 - 0s 847us/step - loss: 17.5520\n",
      "Epoch 203/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 18.89 - ETA: 0s - loss: 20.58 - ETA: 0s - loss: 16.83 - ETA: 0s - loss: 16.45 - ETA: 0s - loss: 17.70 - 0s 796us/step - loss: 17.1685\n",
      "Epoch 204/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 18.08 - ETA: 0s - loss: 18.36 - ETA: 0s - loss: 17.06 - ETA: 0s - loss: 18.00 - ETA: 0s - loss: 16.80 - ETA: 0s - loss: 17.38 - 0s 843us/step - loss: 17.0517\n",
      "Epoch 205/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 14.06 - ETA: 0s - loss: 21.36 - ETA: 0s - loss: 17.18 - ETA: 0s - loss: 16.87 - ETA: 0s - loss: 16.18 - ETA: 0s - loss: 16.82 - 0s 919us/step - loss: 16.7774\n",
      "Epoch 206/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 9.468 - ETA: 0s - loss: 11.91 - ETA: 0s - loss: 15.32 - ETA: 0s - loss: 15.29 - ETA: 0s - loss: 16.41 - 0s 852us/step - loss: 16.5156\n",
      "Epoch 207/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 8.386 - ETA: 0s - loss: 18.41 - ETA: 0s - loss: 14.79 - ETA: 0s - loss: 17.38 - ETA: 0s - loss: 16.73 - ETA: 0s - loss: 16.12 - 0s 847us/step - loss: 16.2084\n",
      "Epoch 208/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 10.11 - ETA: 0s - loss: 15.27 - ETA: 0s - loss: 14.29 - ETA: 0s - loss: 14.97 - ETA: 0s - loss: 15.41 - ETA: 0s - loss: 16.87 - ETA: 0s - loss: 16.07 - 0s 944us/step - loss: 16.0048\n",
      "Epoch 209/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 18.09 - ETA: 0s - loss: 12.40 - ETA: 0s - loss: 13.19 - ETA: 0s - loss: 14.07 - ETA: 0s - loss: 13.99 - ETA: 0s - loss: 15.61 - 0s 875us/step - loss: 15.7608\n",
      "Epoch 210/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 14.30 - ETA: 0s - loss: 15.23 - ETA: 0s - loss: 13.34 - ETA: 0s - loss: 14.68 - ETA: 0s - loss: 14.36 - ETA: 0s - loss: 14.23 - 0s 880us/step - loss: 15.5405\n",
      "Epoch 211/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 8.240 - ETA: 0s - loss: 12.35 - ETA: 0s - loss: 15.74 - ETA: 0s - loss: 16.48 - ETA: 0s - loss: 15.92 - ETA: 0s - loss: 15.56 - 0s 824us/step - loss: 15.5050\n",
      "Epoch 212/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 6.302 - ETA: 0s - loss: 9.529 - ETA: 0s - loss: 11.35 - ETA: 0s - loss: 12.99 - ETA: 0s - loss: 14.15 - 0s 801us/step - loss: 15.1174\n",
      "Epoch 213/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 16.63 - ETA: 0s - loss: 14.25 - ETA: 0s - loss: 13.31 - ETA: 0s - loss: 13.04 - ETA: 0s - loss: 14.88 - ETA: 0s - loss: 14.63 - 0s 826us/step - loss: 14.8785\n",
      "Epoch 214/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 24.57 - ETA: 0s - loss: 17.70 - ETA: 0s - loss: 18.60 - ETA: 0s - loss: 16.28 - ETA: 0s - loss: 15.71 - ETA: 0s - loss: 15.20 - ETA: 0s - loss: 14.94 - 0s 1ms/step - loss: 14.6255\n",
      "Epoch 215/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 17.62 - ETA: 0s - loss: 11.48 - ETA: 0s - loss: 12.63 - ETA: 0s - loss: 13.94 - ETA: 0s - loss: 14.75 - ETA: 0s - loss: 14.27 - 0s 889us/step - loss: 14.3768\n",
      "Epoch 216/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 9.873 - ETA: 0s - loss: 13.82 - ETA: 0s - loss: 14.09 - ETA: 0s - loss: 12.87 - ETA: 0s - loss: 13.69 - ETA: 0s - loss: 13.80 - ETA: 0s - loss: 13.99 - 0s 1ms/step - loss: 14.1546\n",
      "Epoch 217/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 17.20 - ETA: 0s - loss: 16.26 - ETA: 0s - loss: 14.20 - ETA: 0s - loss: 15.35 - ETA: 0s - loss: 14.92 - ETA: 0s - loss: 14.16 - 0s 845us/step - loss: 13.9942\n",
      "Epoch 218/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 16.74 - ETA: 0s - loss: 11.90 - ETA: 0s - loss: 13.48 - ETA: 0s - loss: 13.49 - ETA: 0s - loss: 13.93 - ETA: 0s - loss: 14.05 - 0s 887us/step - loss: 13.7801\n",
      "Epoch 219/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 9.919 - ETA: 0s - loss: 10.76 - ETA: 0s - loss: 14.17 - ETA: 0s - loss: 14.46 - ETA: 0s - loss: 13.25 - ETA: 0s - loss: 13.01 - 0s 866us/step - loss: 13.6848\n",
      "Epoch 220/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 10.38 - ETA: 0s - loss: 15.68 - ETA: 0s - loss: 13.31 - ETA: 0s - loss: 12.86 - ETA: 0s - loss: 14.01 - ETA: 0s - loss: 13.61 - 0s 861us/step - loss: 13.4595\n",
      "Epoch 221/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 15.47 - ETA: 0s - loss: 12.57 - ETA: 0s - loss: 13.53 - ETA: 0s - loss: 13.47 - ETA: 0s - loss: 14.55 - ETA: 0s - loss: 13.16 - ETA: 0s - loss: 13.49 - 0s 1ms/step - loss: 13.1987\n",
      "Epoch 222/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 12.48 - ETA: 0s - loss: 15.46 - ETA: 0s - loss: 13.31 - ETA: 0s - loss: 12.36 - ETA: 0s - loss: 14.32 - ETA: 0s - loss: 14.59 - ETA: 0s - loss: 14.61 - ETA: 0s - loss: 14.13 - ETA: 0s - loss: 14.08 - 1s 2ms/step - loss: 13.0272\n",
      "Epoch 223/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 5.573 - ETA: 0s - loss: 14.94 - ETA: 0s - loss: 15.49 - ETA: 0s - loss: 14.56 - ETA: 0s - loss: 13.63 - ETA: 0s - loss: 12.94 - ETA: 0s - loss: 13.08 - ETA: 0s - loss: 12.90 - 1s 1ms/step - loss: 12.8468\n",
      "Epoch 224/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 14.46 - ETA: 0s - loss: 10.33 - ETA: 0s - loss: 10.85 - ETA: 0s - loss: 10.61 - ETA: 0s - loss: 11.65 - ETA: 0s - loss: 11.58 - ETA: 0s - loss: 12.26 - 0s 977us/step - loss: 12.7120\n",
      "Epoch 225/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 16.45 - ETA: 0s - loss: 12.65 - ETA: 0s - loss: 14.18 - ETA: 0s - loss: 12.92 - ETA: 0s - loss: 12.76 - ETA: 0s - loss: 12.42 - 0s 829us/step - loss: 12.5450\n",
      "Epoch 226/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 11.78 - ETA: 0s - loss: 12.61 - ETA: 0s - loss: 12.01 - ETA: 0s - loss: 10.93 - ETA: 0s - loss: 11.15 - ETA: 0s - loss: 11.80 - 0s 847us/step - loss: 12.3842\n",
      "Epoch 227/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 8.231 - ETA: 0s - loss: 12.03 - ETA: 0s - loss: 12.61 - ETA: 0s - loss: 14.10 - ETA: 0s - loss: 13.04 - ETA: 0s - loss: 12.39 - 0s 887us/step - loss: 12.3123\n",
      "Epoch 228/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 22.31 - ETA: 0s - loss: 14.30 - ETA: 0s - loss: 14.29 - ETA: 0s - loss: 13.02 - ETA: 0s - loss: 12.70 - ETA: 0s - loss: 11.72 - ETA: 0s - loss: 11.77 - 0s 1ms/step - loss: 12.1221\n",
      "Epoch 229/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 15.47 - ETA: 0s - loss: 12.25 - ETA: 0s - loss: 11.79 - ETA: 0s - loss: 12.46 - ETA: 0s - loss: 12.02 - ETA: 0s - loss: 11.68 - ETA: 0s - loss: 11.51 - 0s 1ms/step - loss: 11.9487\n",
      "Epoch 230/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 7.657 - ETA: 0s - loss: 11.17 - ETA: 0s - loss: 9.3348 - ETA: 0s - loss: 12.13 - ETA: 0s - loss: 12.81 - ETA: 0s - loss: 12.52 - ETA: 0s - loss: 11.99 - 0s 905us/step - loss: 11.7571\n",
      "Epoch 231/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 12.18 - ETA: 0s - loss: 10.75 - ETA: 0s - loss: 11.85 - ETA: 0s - loss: 13.76 - ETA: 0s - loss: 13.12 - ETA: 0s - loss: 12.50 - 0s 960us/step - loss: 11.6687\n",
      "Epoch 232/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 11.66 - ETA: 0s - loss: 10.55 - ETA: 0s - loss: 8.9926 - ETA: 0s - loss: 10.30 - ETA: 0s - loss: 10.84 - ETA: 0s - loss: 10.55 - ETA: 0s - loss: 11.08 - ETA: 0s - loss: 11.51 - 1s 1ms/step - loss: 11.5166\n",
      "Epoch 233/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 8.811 - ETA: 0s - loss: 7.457 - ETA: 0s - loss: 13.00 - ETA: 0s - loss: 12.57 - ETA: 0s - loss: 12.78 - ETA: 0s - loss: 12.70 - ETA: 0s - loss: 12.42 - ETA: 0s - loss: 11.42 - 1s 1ms/step - loss: 11.4301\n",
      "Epoch 234/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 2.154 - ETA: 0s - loss: 10.94 - ETA: 0s - loss: 10.29 - ETA: 0s - loss: 11.76 - ETA: 0s - loss: 11.35 - ETA: 0s - loss: 10.49 - 0s 882us/step - loss: 11.2894\n",
      "Epoch 235/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 8.985 - ETA: 0s - loss: 14.06 - ETA: 0s - loss: 13.16 - ETA: 0s - loss: 11.32 - ETA: 0s - loss: 10.82 - ETA: 0s - loss: 10.91 - ETA: 0s - loss: 11.15 - 0s 970us/step - loss: 11.1728\n",
      "Epoch 236/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 10.80 - ETA: 0s - loss: 10.43 - ETA: 0s - loss: 10.61 - ETA: 0s - loss: 11.79 - ETA: 0s - loss: 11.19 - ETA: 0s - loss: 11.11 - ETA: 0s - loss: 10.84 - 1s 1ms/step - loss: 10.9837\n",
      "Epoch 237/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 7.235 - ETA: 0s - loss: 11.11 - ETA: 0s - loss: 12.61 - ETA: 0s - loss: 11.50 - ETA: 0s - loss: 10.21 - ETA: 0s - loss: 10.58 - ETA: 0s - loss: 10.84 - 0s 1ms/step - loss: 10.8845\n",
      "Epoch 238/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 6.860 - ETA: 0s - loss: 9.624 - ETA: 0s - loss: 8.270 - ETA: 0s - loss: 9.382 - ETA: 0s - loss: 10.44 - ETA: 0s - loss: 11.28 - 0s 861us/step - loss: 10.7591\n",
      "Epoch 239/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 9.680 - ETA: 0s - loss: 11.81 - ETA: 0s - loss: 10.05 - ETA: 0s - loss: 10.93 - ETA: 0s - loss: 10.88 - ETA: 0s - loss: 11.11 - ETA: 0s - loss: 10.73 - 0s 990us/step - loss: 10.6425\n",
      "Epoch 240/1000\n",
      "432/432 [==============================] - ETA: 0s - loss: 4.369 - ETA: 0s - loss: 9.208 - ETA: 0s - loss: 8.252 - ETA: 0s - loss: 9.458 - ETA: 0s - loss: 9.601 - ETA: 0s - loss: 10.08 - ETA: 0s - loss: 10.06 - 0s 988us/step - loss: 10.6765\n",
      "Epoch 00240: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x124f8102088>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)\n",
    "\n",
    "model.fit(X, y, epochs=1000,\n",
    "          batch_size=30, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_2.pkl']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'model_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델2 적용 함수\n",
    "def model_fit_2(data1, data2, model_name, file_name):\n",
    "    data=pd.concat([data1.reset_index(drop=True), data2.reset_index(drop=True)], axis=1)\n",
    "    model = joblib.load(str(model_name))\n",
    "    data_r = data.values.reshape(data.shape[0],data.shape[1],1 )\n",
    "    pred_out=model.predict(data_r)\n",
    "    df = pd.DataFrame({'id':range(144*33, 144*113),\n",
    "              'Y18':pred_out.reshape(1,-1)[0]})\n",
    "    df.to_csv(file_name, index = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out_1 = model_fit_1(test, 'model_1.pkl', 'pred_out_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out_fin = model_fit_2(test, pred_out_1, 'model_2.pkl', 'pred_out_fin.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
